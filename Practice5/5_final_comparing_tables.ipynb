{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Сводная таблица: модели с учителем\n",
    "\n",
    "| Метод                                                                  | Тип задачи | Основная идея / что моделирует | Когда применять | Плюсы | Минусы | Актуальность / роль сейчас |\n",
    "|------------------------------------------------------------------------|---|---|---|---|---|---|\n",
    "| Линейная регрессия (МНК=OLS)                                           | Регрессия | Линейная модель: $\\hat y = w^\\top x + b$, параметры подбираются минимизацией среднеквадратичной ошибки (MSE) | Базовый старт для числовых признаков, приблизительно линейных зависимостей; задачи, где важна интерпретация вкладов признаков | Простая, быстрая; есть аналитическое решение; легко интерпретировать коэффициенты | Плохо ловит нелинейности и взаимодействия; чувствительна к выбросам и мультиколлинеарности; может переобучаться при $d \\gg n$ | Классический статистический инструмент и baseline; как финальная модель в сложных табличных задачах часто уступает ансамблям деревьев |\n",
    "| Регуляризованные линейные модели (Ridge / Lasso)                       | Регрессия, классификация (через логистическую регрессию) | Та же линейная модель, но к функции потерь добавляется штраф: Ridge — $L_2$-норма весов, Lasso — $L_1$-норма; контролируют сложность модели и величину весов | Высокомерные задачи (много признаков), особенно разреженные (тексты, рекомендательные фичи); когда нужны устойчивые и/или разреженные решения (Lasso) | Лучше контролируют переобучение, чем OLS; Ridge стабилизирует решение при мультиколлинеарности; Lasso автоматически отбирает признаки (зануляет веса) | Всё ещё линейная модель, не ловит сложные нелинейности и сильные взаимодействия признаков | Очень актуальны: стандарт для линейных моделей в индустрии (тексты, рекламе, скоринговые модели), сильный baseline до сих пор |\n",
    "| Логистическая регрессия (в т.ч. softmax)                               | Классификация (бинарная и мультикласс) | Линейный скор $z = w^\\top x + b$, переводим в вероятность через сигмоиду или softmax, параметры подбираются максимизацией правдоподобия (минимизация логистической потери) | Бинарная и мультиклассовая классификация, особенно при разреженных признаках и когда нужны калиброванные вероятности и интерпретируемые коэффициенты | Простая, быстрая; хорошо работает на разреженных фичах; вероятностный вывод; понятные веса признаков | Линейный разделяющий гиперплоскостью классификатор; плохо работает при сложных нелинейностях; чувствительна к масштабированию и выбросам | До сих пор один из стандартных baseline’ов; во многих регуляторно жёстких областях (скоринг, медицина) часто используется как продовая модель |\n",
    "|k-ближайших соседей (kNN)                                               | Классификация и регрессия | Решение для нового объекта строится по $k$ ближайшим тренировочным точкам: голосование по классам или среднее по таргету; никакого явного обучения (instance-based learning) | Очень небольшие датасеты, задачи, где метрика расстояния хорошо отражает “похожесть”; быстрый sanity-check и учебные примеры | Концептуально прост; впитывает сложные нелинейности за счёт локальности; нет явной фазы обучения | Плохо масштабируется по числу объектов (медленный предикт); чувствителен к масштабу признаков и выбору метрики; “проклятие размерности” | В реальных табличных задачах редко используется как финальная модель; в основном учебный пример и простой baseline, иногда — внутренняя часть поиска по эмбеддингам |\n",
    "| Дерево решений                                                         | Классификация и регрессия | Рекурсивное разбиение пространства признаков по порогам: в каждой вершине выбирается признак и порог, максимально уменьшающие impurity (Gini, энтропия, MSE) | Когда нужна простая интерпретируемая модель (небольшое дерево); как базовый building block для ансамблей (Random Forest, бустинг) | Естественно ловит нелинейности и взаимодействия; умеет работать с числовыми и категориальными признаками; почти не требует масштабирования признаков | Склонно к жёсткому переобучению; сильно нестабильно к небольшим изменениям данных; как одиночная модель не даёт SoTA-качества | Как одиночная модель — в основном для интерпретации и учебных примеров; как базовый алгоритм в ансамблях — критически важен и супер-актуален |\n",
    "| Случайный лес (Random Forest)                                          | Классификация и регрессия | Ансамбль деревьев, обученных на бутстрап-выборках с случайным подмножеством признаков; усреднение предсказаний по деревьям | Универсальный сильный baseline на табличных данных; когда важно “чуть меньше тюнинга и чуть больше стабильности”, чем у бустинга | Довольно устойчив к переобучению; хорошо работает “из коробки”; менее чувствителен к настройкам, чем бустинг; умеет давать важности признаков | Менее точен, чем хорошо настроенный градиентный бустинг на сложных задачах; относительно тяжёлый по памяти | Очень актуальный классический метод на табличных данных; в некоторых задачах вполне конкурентен бустингу, часто используется как надёжный baseline |\n",
    "| Градиентный бустинг над деревьями (XGBoost / LightGBM / CatBoost и аналоги) | Классификация и регрессия | Ансамбль из “слабых” деревьев, добавляемых последовательно; каждое новое дерево аппроксимирует градиент (или псевдо-остатки) функции потерь по текущему ансамблю | Основной рабочий инструмент для сложных табличных задач (кредитный скоринг, риск, маркетинг, CTR, табличные Kaggle-соревнования и т.п.) | Очень высокая точность на табличных данных; хорошо ловит сложные нелинейности и взаимодействия; поддержка категориальных признаков (CatBoost), пропусков, неидеальных данных | Много гиперпараметров; чувствителен к тюнингу; тяжелее в обучении, чем линейные модели и Random Forest; в очень больших по $n$ задачах может быть тяжёлым | Один из де-факто SoTA-подходов для табличных данных; обязательный инструмент в арсенале практикующего ML-инженера |\n",
    "| SVM (линейный и ядерный)                                               | Классификация и регрессия (SVR) | Линейный классификатор/регрессор, который максимизирует геометрический зазор (margin) между классами; с ядрами использует kernel trick и работает в скрытом пространстве признаков | Небольшие и средние по размеру датасеты, особенно высокомерные (тексты, фичи от эмбеддингов), когда надо хорошее качество при ограниченном количестве данных | Сильный теоретический фундамент; хорошо работает на небольших/средних выборках; с ядрами может моделировать сложные разделяющие поверхности | Плохо масштабируется по числу объектов; дорогой предикт при большом числе опорных векторов; чувствителен к выбору ядра и гиперпараметров $C$, $\\gamma$ | Сейчас больше нишевый метод и учебный стандарт; в проде применяется гораздо реже, чем бустинг/деревья/линейные модели, но остаётся полезным инструментом на небольших задачах |\n"
   ],
   "id": "beae6b33ecce768e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Сводная таблица: кластеризация и уменьшение размерности\n",
    "\n",
    "| Метод | Тип задачи | Основная идея / что моделирует | Когда применять | Плюсы | Минусы | Актуальность / роль сейчас |\n",
    "|---|---|---|---|---|---|---|\n",
    "| PCA (анализ главных компонент) | Уменьшение размерности, препроцессинг, визуализация | Линейное ортогональное преобразование признаков; находит направления (главные компоненты), вдоль которых дисперсия максимальна; проектируем данные в пространство первых компонент | Снижение размерности перед обучением моделей; борьба с мультиколлинеарностью; шумоподавление; первичная 2D/3D-визуализация структуры данных | Быстрый, детерминированный и хорошо понятный метод; даёт интерпретируемые компоненты; улучшает численную устойчивость моделей | Линейный метод: не ловит сложные нелинейные многообразия; иногда компоненты трудно интерпретировать в терминах исходных признаков | До сих пор стандартный инструмент препроцессинга и анализа; очень актуален, особенно как базовый линейный метод уменьшения размерности |\n",
    "| t-SNE | Нелинейное уменьшение размерности, визуализация | Строит такое низкоразмерное отображение, чтобы сохранять вероятностную структуру локальных соседств: близкие точки в исходном пространстве остаются близкими в 2D/3D | Визуализация высокоразмерных данных (эмбеддинги, признаки) для поиска кластерной структуры, аномалий, подгрупп; только для “красивых картинок” и анализа | Отлично показывает локальную кластерную структуру; очень популярен как инструмент визуального анализа эмбеддингов | Дорог по времени ($O(n^2)$), плохо масштабируется; много гиперпараметров; результат не предназначен как вход в последующие модели; плохо переносится на новые точки | Всё ещё широко используется для исследовательской визуализации (особенно в статьях), но в продовых пайплайнах как отдельный этап уменьшения размерности применяется редко |\n",
    "| UMAP | Нелинейное уменьшение размерности, визуализация, иногда препроцессинг | Строит отображение, стремясь сохранить топологическую структуру данных (граф соседства) в низкомерном пространстве; чаще даёт более стабильную глобальную структуру, чем t-SNE | Визуализация высокоразмерных данных; предварительное уменьшение размерности перед кластеризацией или обучением моделей; работа с большими по $n$ наборами признаков | Обычно быстрее и лучше масштабируется, чем t-SNE; часто лучше сохраняет глобальную геометрию; может использоваться как реальный шаг препроцессинга | Результат зависит от выбора метрики и гиперпараметров; интерпретация осей сложна; строгих теоретических гарантий качества для конкретной задачи почти нет | Очень актуальный метод для визуализации эмбеддингов и предварительного уменьшения размерности; фактически “современный стандарт” нелинейного уменьшения размерности в applied-части |\n",
    "| $k$-means | Кластеризация | Разбивает объекты на $K$ кластеров, минимизируя сумму квадратов расстояний до центров кластеров; каждый кластер описывается центроидом, принадлежность — по ближайшему центру | Сегментация пользователей/объектов, когда предполагается примерно “сферическая” структура кластеров; быстрый baseline для кластеризации; предварительное разбиение перед более сложными моделями | Простой и быстрый; хорошо масштабируется; легко интерпретировать центры кластеров как “средний профиль”; множество реализаций и расширений | Требует заранее задать $K$; чувствителен к инициализации и масштабированию признаков; плохо работает при кластерах сложной формы или сильно различной плотности; чувствителен к выбросам | Базовый и до сих пор один из самых используемых методов кластеризации; в проде часто применяется как быстрый рабочий инструмент сегментации и предварительного анализа |\n",
    "| DBSCAN | Кластеризация (плотностная), выявление шума | Определяет кластеры как области повышенной плотности: ядровые точки с достаточным числом соседей в радиусе $\\varepsilon$ образуют кластеры, всё вне плотных областей считается шумом | Когда кластеры могут быть произвольной формы; когда важны выбросы (шумовые точки); задачи геопространственных данных, траекторий, точек в $\\mathbb{R}^2$ / $\\mathbb{R}^3$ | Не требует заранее задавать число кластеров; умеет находить кластеры произвольной формы; явно выделяет шумовые точки | Чувствителен к выбору $\\varepsilon$ и `min_samples`; плохо работает при сильно различающейся плотности кластеров; в высокой размерности расстояния теряют информативность | Актуален как базовый плотностной метод для кластеризации и поиска аномалий в низко/среднеразмерных данных; в более сложных сценариях часто заменяется продвинутыми вариантами (OPTICS, HDBSCAN), но концептуально остаётся важным |\n"
   ],
   "id": "b4658adf5e008b8f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "99a9cb58a664648f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
